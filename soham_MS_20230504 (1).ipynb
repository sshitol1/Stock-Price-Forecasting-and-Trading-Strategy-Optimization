{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bidOEqGG3xNE"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import statistics\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OtCiMa_3cRQ",
        "outputId": "2722c469-281c-4e0b-84ca-ffd7d423ec87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "ln: failed to create symbolic link '/mydrive/My Drive': File exists\n",
            "'Celebrate Good Times - 6 Jun 2022.MOV'\n",
            " Code_assessment.gdoc\n",
            " Code.gdoc\n",
            "'Colab Notebooks'\n",
            "'CS 571 Midterm.pdf'\n",
            "'DataMining (1).zip'\n",
            " DataMining.zip\n",
            " design-inits.sh\n",
            " Di_Lagna_2022\n",
            " Diwali2021\n",
            "'dtm-jfad-iwb â€“ 21 Oct 2022.gjam'\n",
            "'Graduation Ceremony of 2021-22 pass out.xlsx'\n",
            " haskell.7z\n",
            " Homework_1_Solution.pdf\n",
            " Homework_2_Solution.pdf\n",
            " IMG_4444.JPG\n",
            " main.py\n",
            "'My Drive'\n",
            " Names.pdf\n",
            " parser.py\n",
            " PL_Final.docx\n",
            " PL_Final.pdf\n",
            " Pl_HW2.pages\n",
            " PL_HW.docx\n",
            " PL_HW.pdf\n",
            " PLmidterm.pages\n",
            " PLmidterm.pdf\n",
            " Pl.zip\n",
            " prj3-sol\n",
            " prj3-sol-32.zip\n",
            " prj4-sol.hs\n",
            " prj5_sol.erl\n",
            " Programming_Language_Syntax.pdf\n",
            "'Project 3.pdf'\n",
            " Scheme.pdf\n",
            " soham\n",
            " SOHAM\n",
            " soham_MS_V2.ipynb\n",
            " Submission.gdoc\n",
            "'Untitled document.gdoc'\n",
            " Vishal\n",
            " Words.gslides\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ln -s /content/gdrive/My\\ Drive/ /mydrive\n",
        "!ls /mydrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EPx6mTEJHj7"
      },
      "outputs": [],
      "source": [
        "with open(\"/mydrive/soham/training_set.pkl\", \"rb\") as f:\n",
        "    df = pickle.load(f)\n",
        "\n",
        "data = []\n",
        "number_of_instances = 1500\n",
        "\n",
        "for i in range(number_of_instances):\n",
        "  data.append(df[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2_fqVleXa0w"
      },
      "outputs": [],
      "source": [
        "# data = pd.read_csv(file)\n",
        "# increase = 0\n",
        "# decrease = 1\n",
        "# no big change = 2\n",
        "\n",
        "for ii in range(number_of_instances):\n",
        "  Close = data[ii]['Close']\n",
        "  Open = data[ii]['Open']\n",
        "  High = data[ii]['High']\n",
        "  Low = data[ii]['Low']\n",
        "  Volume = data[ii]['Volume']\n",
        "\n",
        "\n",
        "  Change = [0]* len(Close)\n",
        "  Momentum = [0]* len(Close)\n",
        "  Momentum[0] = 2\n",
        "\n",
        "  for i in range (1,len(Close)):\n",
        "      if Close[i] - Close[i-1] > 0.005:\n",
        "          Momentum[i] = 0\n",
        "          Change[i] = (Close[i]-Close[i-1])/Close[i-1]\n",
        "      elif Close[i-1] - Close[i] > 0.005 :\n",
        "          Momentum[i] = 1\n",
        "          Change[i] = (Close[i-1] - Close[i]) / Close[i - 1]\n",
        "      else:\n",
        "          if Close[i] > Close[i-1] :\n",
        "              Momentum[i] = 2\n",
        "              Change[i] = (Close[i]-Close[i-1])/Close[i-1]\n",
        "          else :\n",
        "              Momentum[i] = 2\n",
        "              Change[i] = (Close[i-1] - Close[i]) / Close[i - 1]\n",
        "\n",
        "  xl = pd.DataFrame({'Open':Open,'High':High,'Low':Low,'Volume':Volume,'Close':Close,'Change':Change,'Momentum':Momentum})\n",
        "  xl.to_csv(\"DATA\" + str(ii)  + \".csv\",index=False,header=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "id": "lYkft0Kzr9VO",
        "outputId": "a64dce55-fdc0-435c-a079-9b9caa133d6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/statistics.py:428: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  T, total, count = _sum(w / x if w else 0 for w, x in zip(weights, data))\n",
            "<ipython-input-6-3ea9563643ac>:49: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
            "  k = stats.describe(close_array)\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:1522: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
            "  sk = skew(a, axis, bias=bias)\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:1523: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
            "  kurt = kurtosis(a, axis, bias=bias)\n",
            "<ipython-input-6-3ea9563643ac>:96: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
            "  k = stats.describe(high_array)\n",
            "<ipython-input-6-3ea9563643ac>:73: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
            "  k = stats.describe(open_array)\n",
            "<ipython-input-6-3ea9563643ac>:121: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
            "  k = stats.describe(low_array)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-3ea9563643ac>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m       \u001b[0mf58\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpstdev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m       \u001b[0mf59\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpvariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m       \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m       \u001b[0mf60\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m       \u001b[0mf61\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py\u001b[0m in \u001b[0;36mdescribe\u001b[0;34m(a, axis, ddof, bias, nan_policy)\u001b[0m\n\u001b[1;32m   1521\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mddof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m     \u001b[0msk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1523\u001b[0;31m     \u001b[0mkurt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkurtosis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1525\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mDescribeResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkurt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_axis_nan_policy.py\u001b[0m in \u001b[0;36maxis_nan_policy_wrapper\u001b[0;34m(_no_deco, *args, **kwds)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;31m# Check for intersection between positional and keyword args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypotest_fun_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0;31m# Give unique names to each positional sample argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36msignature\u001b[0;34m(obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3252\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3253\u001b[0m     \u001b[0;34m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3254\u001b[0;31m     return Signature.from_callable(obj, follow_wrapped=follow_wrapped,\n\u001b[0m\u001b[1;32m   3255\u001b[0m                                    globals=globals, locals=locals, eval_str=eval_str)\n\u001b[1;32m   3256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mfrom_callable\u001b[0;34m(cls, obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3000\u001b[0m                       follow_wrapped=True, globals=None, locals=None, eval_str=False):\n\u001b[1;32m   3001\u001b[0m         \u001b[0;34m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3002\u001b[0;31m         return _signature_from_callable(obj, sigcls=cls,\n\u001b[0m\u001b[1;32m   3003\u001b[0m                                         \u001b[0mfollow_wrapper_chains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3004\u001b[0m                                         globals=globals, locals=locals, eval_str=eval_str)\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2461\u001b[0m         \u001b[0;31m# If it's a pure Python function, or an object that is duck type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2462\u001b[0m         \u001b[0;31m# of a Python function (Cython functions, for instance), then:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2463\u001b[0;31m         return _signature_from_function(sigcls, obj,\n\u001b[0m\u001b[1;32m   2464\u001b[0m                                         \u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2465\u001b[0m                                         globals=globals, locals=locals, eval_str=eval_str)\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36m_signature_from_function\u001b[0;34m(cls, func, skip_bound_arg, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         \u001b[0mkind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_POSITIONAL_ONLY\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mposonly_left\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_POSITIONAL_OR_KEYWORD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0mannotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_empty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m         parameters.append(Parameter(name, annotation=annotation,\n\u001b[0m\u001b[1;32m   2326\u001b[0m                                     kind=kind))\n\u001b[1;32m   2327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mposonly_left\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, kind, default, annotation)\u001b[0m\n\u001b[1;32m   2640\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2641\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'value {kind!r} is not a valid Parameter.kind'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2642\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2643\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_VAR_POSITIONAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_VAR_KEYWORD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2644\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{} parameters cannot have default values'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "data2extract_features = []\n",
        "for ii in range(number_of_instances):\n",
        "    data2extract_features.append(pd.read_csv(\"DATA\" + str(ii)  + \".csv\"))\n",
        "\n",
        "ff1,ff2,ff3,ff4,ff5,ff6,ff7,ff8,ff9,ff10 = [],[],[],[],[],[],[],[],[],[]\n",
        "ff11,ff12,ff13,ff14,ff15,ff16,ff17,ff18,ff19,ff20 = [],[],[],[],[],[],[],[],[],[]\n",
        "ff21,ff22,ff23,ff24,ff25,ff26,ff27,ff28,ff29,ff30 = [],[],[],[],[],[],[],[],[],[]\n",
        "ff31,ff32,ff33,ff34,ff35,ff36,ff37,ff38,ff39,ff40 = [],[],[],[],[],[],[],[],[],[]\n",
        "ff41,ff42,ff43,ff44,ff45,ff46,ff47,ff48,ff49,ff50 = [],[],[],[],[],[],[],[],[],[]\n",
        "ff51,ff52,ff53,ff54,ff55,ff56,ff57,ff58,ff59,ff60 = [],[],[],[],[],[],[],[],[],[]\n",
        "ff61,ff62,ff63,ff64,ff65,ff66,ff67,ff68,ff69,ff70 = [],[],[],[],[],[],[],[],[],[]\n",
        "ff71,ff72,ff73,ff74,ff75,ff76,ff77,ff78,ff79,ff80 = [],[],[],[],[],[],[],[],[],[]\n",
        "ff81,ff82,ff83,ff84,ff85,ff86,ff87,ff88,ff89,ff90 = [],[],[],[],[],[],[],[],[],[]\n",
        "ff91,ff92,ff93,ff94,ff95,ff96,ff97 = [],[],[],[],[],[],[]\n",
        "\n",
        "for ii in range(number_of_instances):\n",
        "  # for i in range(5,15):\n",
        "  for i in range(5,len(data2extract_features[0])):\n",
        "      f1 = (data2extract_features[ii]['Close'][i] + data2extract_features[ii]['Close'][i-1] + data2extract_features[ii]['Close'][i-2] + data2extract_features[ii]['Close'][i-3] + data2extract_features[ii]['Close'][i-4])/5\n",
        "      f2 = (data2extract_features[ii]['Open'][i] + data2extract_features[ii]['Open'][i-1] + data2extract_features[ii]['Open'][i-2] + data2extract_features[ii]['Open'][i-3] + data2extract_features[ii]['Open'][i-4])/5\n",
        "      f3 = (data2extract_features[ii]['High'][i] + data2extract_features[ii]['High'][i-1] + data2extract_features[ii]['High'][i-2] + data2extract_features[ii]['High'][i-3] + data2extract_features[ii]['High'][i-4])/5\n",
        "      f4 = (data2extract_features[ii]['Low'][i] + data2extract_features[ii]['Low'][i-1] + data2extract_features[ii]['Low'][i-2] + data2extract_features[ii]['Low'][i-3] + data2extract_features[ii]['Low'][i-4])/5\n",
        "      f5 = (data2extract_features[ii]['Volume'][i] + data2extract_features[ii]['Volume'][i-1] + data2extract_features[ii]['Volume'][i-2] + data2extract_features[ii]['Volume'][i-3] + data2extract_features[ii]['Volume'][i-4])/5\n",
        "      f6 = (data2extract_features[ii]['Change'][i] + data2extract_features[ii]['Change'][i-1] + data2extract_features[ii]['Change'][i-2] + data2extract_features[ii]['Change'][i-3] + data2extract_features[ii]['Change'][i-4])/5\n",
        "\n",
        "\n",
        "      # close_array = [data2extract_features[ii]['Close'][i] + data2extract_features[ii]['Close'][i-1] + data2extract_features[ii]['Close'][i-2] + data2extract_features[ii]['Close'][i-3] + data2extract_features[ii]['Close'][i-4]]\n",
        "      # open_array = [data2extract_features[ii]['Open'][i] + data2extract_features[ii]['Open'][i-1] + data2extract_features[ii]['Open'][i-2] + data2extract_features[ii]['Open'][i-3] + data2extract_features[ii]['Open'][i-4]]\n",
        "      # high_array = [data2extract_features[ii]['High'][i] + data2extract_features[ii]['High'][i-1] + data2extract_features[ii]['High'][i-2] + data2extract_features[ii]['High'][i-3] + data2extract_features[ii]['High'][i-4]]\n",
        "      # low_array = [data2extract_features[ii]['Low'][i] + data2extract_features[ii]['Low'][i-1] + data2extract_features[ii]['Low'][i-2] + data2extract_features[ii]['Low'][i-3] + data2extract_features[ii]['Low'][i-4]]\n",
        "      # volume_array = [data2extract_features[ii]['Volume'][i] + data2extract_features[ii]['Volume'][i-1] + data2extract_features[ii]['Volume'][i-2] + data2extract_features[ii]['Volume'][i-3] + data2extract_features[ii]['Volume'][i-4]]\n",
        "      # change_array = [data2extract_features[ii]['Change'][i] + data2extract_features[ii]['Change'][i-1] + data2extract_features[ii]['Change'][i-2] + data2extract_features[ii]['Change'][i-3] + data2extract_features[ii]['Change'][i-4]]\n",
        "\n",
        "      close_array = [data2extract_features[ii]['Close'][i] , data2extract_features[ii]['Close'][i-1] , data2extract_features[ii]['Close'][i-2] , data2extract_features[ii]['Close'][i-3] , data2extract_features[ii]['Close'][i-4]]\n",
        "      open_array = [data2extract_features[ii]['Open'][i] , data2extract_features[ii]['Open'][i-1] , data2extract_features[ii]['Open'][i-2] , data2extract_features[ii]['Open'][i-3] , data2extract_features[ii]['Open'][i-4]]\n",
        "      high_array = [data2extract_features[ii]['High'][i] , data2extract_features[ii]['High'][i-1] , data2extract_features[ii]['High'][i-2] , data2extract_features[ii]['High'][i-3] , data2extract_features[ii]['High'][i-4]]\n",
        "      low_array = [data2extract_features[ii]['Low'][i] , data2extract_features[ii]['Low'][i-1] , data2extract_features[ii]['Low'][i-2] , data2extract_features[ii]['Low'][i-3] , data2extract_features[ii]['Low'][i-4]]\n",
        "      volume_array = [data2extract_features[ii]['Volume'][i] , data2extract_features[ii]['Volume'][i-1] , data2extract_features[ii]['Volume'][i-2] , data2extract_features[ii]['Volume'][i-3] , data2extract_features[ii]['Volume'][i-4]]\n",
        "      change_array = [data2extract_features[ii]['Change'][i] , data2extract_features[ii]['Change'][i-1] , data2extract_features[ii]['Change'][i-2] , data2extract_features[ii]['Change'][i-3] , data2extract_features[ii]['Change'][i-4]]\n",
        "\n",
        "      f7 = statistics.median(close_array)\n",
        "      f8 = statistics.harmonic_mean(close_array)\n",
        "      f9 = statistics.median_low(close_array)\n",
        "      f10 = statistics.median_high(close_array)\n",
        "      f11 = statistics.median_grouped(close_array)\n",
        "      f12 = statistics.mode(close_array)\n",
        "      f13 = statistics.pstdev(close_array)\n",
        "      f14 = statistics.pvariance(close_array)\n",
        "      k = stats.describe(close_array)\n",
        "      f15 = k[1][0]\n",
        "      f16 = k[1][1]\n",
        "      f17 = k[2]\n",
        "      f18 = k[3]\n",
        "      if math.isnan(f18):\n",
        "        f18 = 0\n",
        "      f19 = k[4]\n",
        "      if math.isnan(f19):\n",
        "        f19 = 0\n",
        "      f20 = k[5]\n",
        "      if math.isnan(f20):\n",
        "        f20 = 0\n",
        "      f21 = k[2]\n",
        "\n",
        "#\n",
        "      f22 = statistics.median(open_array)\n",
        "      f23 = statistics.harmonic_mean(open_array)\n",
        "      f24 = statistics.median_low(open_array)\n",
        "      f25 = statistics.median_high(open_array)\n",
        "      f26 = statistics.median_grouped(open_array)\n",
        "      f27 = statistics.mode(open_array)\n",
        "      f28 = statistics.pstdev(open_array)\n",
        "      f29 = statistics.pvariance(open_array)\n",
        "      k = stats.describe(open_array)\n",
        "      f30 = k[1][0]\n",
        "      f31 = k[1][1]\n",
        "      f32 = k[2]\n",
        "      f33 = k[3]\n",
        "      if math.isnan(f33):\n",
        "        f33 = 0\n",
        "      f34 = k[4]\n",
        "      if math.isnan(f34):\n",
        "        f34 = 0\n",
        "      f35 = k[5]\n",
        "      if math.isnan(f35):\n",
        "        f35 = 0\n",
        "      f36 = k[2]\n",
        "#\n",
        "      f37 = statistics.median(high_array)\n",
        "      f38 = statistics.harmonic_mean(high_array)\n",
        "      f39 = statistics.median_low(high_array)\n",
        "      f40 = statistics.median_high(high_array)\n",
        "      f41 = statistics.median_grouped(high_array)\n",
        "      f42 = statistics.mode(high_array)\n",
        "      f43 = statistics.pstdev(high_array)\n",
        "      f44 = statistics.pvariance(high_array)\n",
        "      k = stats.describe(high_array)\n",
        "      f45 = k[1][0]\n",
        "      f46 = k[1][1]\n",
        "      f47 = k[2]\n",
        "      f48 = k[3]\n",
        "      if math.isnan(f48):\n",
        "        f48 = 0\n",
        "      f49 = k[4]\n",
        "      if math.isnan(f49):\n",
        "        f49 = 0\n",
        "      f50 = k[5]\n",
        "      if math.isnan(f50):\n",
        "        f50 = 0\n",
        "      f51 = k[2]\n",
        "\n",
        "\n",
        "#\n",
        "      f52 = statistics.median(low_array)\n",
        "      f53 = statistics.harmonic_mean(low_array)\n",
        "      f54 = statistics.median_low(low_array)\n",
        "      f55 = statistics.median_high(low_array)\n",
        "      f56 = statistics.median_grouped(low_array)\n",
        "      f57 = statistics.mode(low_array)\n",
        "      f58 = statistics.pstdev(low_array)\n",
        "      f59 = statistics.pvariance(low_array)\n",
        "      k = stats.describe(low_array)\n",
        "      f60 = k[1][0]\n",
        "      f61 = k[1][1]\n",
        "      f62 = k[2]\n",
        "      f63 = k[3]\n",
        "      if math.isnan(f63):\n",
        "        f63 = 0\n",
        "      f64 = k[4]\n",
        "      if math.isnan(f64):\n",
        "        f64 = 0\n",
        "      f65 = k[5]\n",
        "      if math.isnan(f65):\n",
        "        f65 = 0\n",
        "      f66 = k[2]\n",
        "\n",
        "#\n",
        "      f67 = statistics.median(volume_array)\n",
        "      f68 = statistics.harmonic_mean(volume_array)\n",
        "      f69 = statistics.median_low(volume_array)\n",
        "      f70 = statistics.median_high(volume_array)\n",
        "      f71 = statistics.median_grouped(volume_array)\n",
        "      f72 = statistics.mode(volume_array)\n",
        "      f73 = statistics.pstdev(volume_array)\n",
        "      f74 = statistics.pvariance(volume_array)\n",
        "      k = stats.describe(volume_array)\n",
        "      f75 = k[1][0]\n",
        "      f76 = k[1][1]\n",
        "      f77 = k[2]\n",
        "      f78 = k[3]\n",
        "      if math.isnan(f78):\n",
        "        f78 = 0\n",
        "      f79 = k[4]\n",
        "      if math.isnan(f79):\n",
        "        f79 = 0\n",
        "      f80 = k[5]\n",
        "      if math.isnan(f80):\n",
        "        f80 = 0\n",
        "      f81 = k[2]\n",
        "\n",
        "#\n",
        "      f82 = statistics.median(change_array)\n",
        "      f83 = statistics.harmonic_mean(change_array)\n",
        "      f84 = statistics.median_low(change_array)\n",
        "      f85 = statistics.median_high(change_array)\n",
        "      f86 = statistics.median_grouped(change_array)\n",
        "      f87 = statistics.mode(change_array)\n",
        "      f88 = statistics.pstdev(change_array)\n",
        "      f89 = statistics.pvariance(change_array)\n",
        "      k = stats.describe(change_array)\n",
        "      f90 = k[1][0]\n",
        "      f91 = k[1][1]\n",
        "      f92 = k[2]\n",
        "      f93 = k[3]\n",
        "      if math.isnan(f93):\n",
        "        f93 = 0\n",
        "      f94 = k[4]\n",
        "      if math.isnan(f94):\n",
        "        f94 = 0\n",
        "      f95 = k[5]\n",
        "      if math.isnan(f95):\n",
        "        f95 = 0\n",
        "      f96 = k[2]\n",
        "\n",
        "      f97 = data2extract_features[ii]['Momentum'][i-4]\n",
        "\n",
        "      ff1.append(f1)\n",
        "      ff2.append(f2)\n",
        "      ff3.append(f3)\n",
        "      ff4.append(f4)\n",
        "      ff5.append(f5)\n",
        "      ff6.append(f6)\n",
        "      ff7.append(f7)\n",
        "      ff8.append(f8)\n",
        "      ff9.append(f9)\n",
        "      ff10.append(f10)\n",
        "\n",
        "      ff11.append(f11)\n",
        "      ff12.append(f12)\n",
        "      ff13.append(f13)\n",
        "      ff14.append(f14)\n",
        "      ff15.append(f15)\n",
        "      ff16.append(f16)\n",
        "      ff17.append(f17)\n",
        "      ff18.append(f18)\n",
        "      ff19.append(f19)\n",
        "      ff20.append(f20)\n",
        "\n",
        "      ff21.append(f21)\n",
        "      ff22.append(f22)\n",
        "      ff23.append(f23)\n",
        "      ff24.append(f24)\n",
        "      ff25.append(f25)\n",
        "      ff26.append(f26)\n",
        "      ff27.append(f27)\n",
        "      ff28.append(f28)\n",
        "      ff29.append(f29)\n",
        "      ff30.append(f30)\n",
        "\n",
        "      ff31.append(f31)\n",
        "      ff32.append(f32)\n",
        "      ff33.append(f33)\n",
        "      ff34.append(f34)\n",
        "      ff35.append(f35)\n",
        "      ff36.append(f36)\n",
        "      ff37.append(f37)\n",
        "      ff38.append(f38)\n",
        "      ff39.append(f39)\n",
        "      ff40.append(f40)\n",
        "\n",
        "      ff41.append(f41)\n",
        "      ff42.append(f42)\n",
        "      ff43.append(f43)\n",
        "      ff44.append(f44)\n",
        "      ff45.append(f45)\n",
        "      ff46.append(f46)\n",
        "      ff47.append(f47)\n",
        "      ff48.append(f48)\n",
        "      ff49.append(f49)\n",
        "      ff50.append(f50)\n",
        "\n",
        "\n",
        "      ff51.append(f51)\n",
        "      ff52.append(f52)\n",
        "      ff53.append(f53)\n",
        "      ff54.append(f54)\n",
        "      ff55.append(f55)\n",
        "      ff56.append(f56)\n",
        "      ff57.append(f57)\n",
        "      ff58.append(f58)\n",
        "      ff59.append(f59)\n",
        "      ff60.append(f60)\n",
        "\n",
        "\n",
        "      ff61.append(f61)\n",
        "      ff62.append(f62)\n",
        "      ff63.append(f63)\n",
        "      ff64.append(f64)\n",
        "      ff65.append(f65)\n",
        "      ff66.append(f66)\n",
        "      ff67.append(f67)\n",
        "      ff68.append(f68)\n",
        "      ff69.append(f69)\n",
        "      ff70.append(f70)\n",
        "\n",
        "      ff71.append(f71)\n",
        "      ff72.append(f72)\n",
        "      ff73.append(f73)\n",
        "      ff74.append(f74)\n",
        "      ff75.append(f75)\n",
        "      ff76.append(f76)\n",
        "      ff77.append(f77)\n",
        "      ff78.append(f78)\n",
        "      ff79.append(f79)\n",
        "      ff80.append(f80)\n",
        "\n",
        "      ff81.append(f81)\n",
        "      ff82.append(f82)\n",
        "      ff83.append(f83)\n",
        "      ff84.append(f84)\n",
        "      ff85.append(f85)\n",
        "      ff86.append(f86)\n",
        "      ff87.append(f87)\n",
        "      ff88.append(f88)\n",
        "      ff89.append(f89)\n",
        "      ff90.append(f90)\n",
        "\n",
        "      ff91.append(f91)\n",
        "      ff92.append(f92)\n",
        "      ff93.append(f93)\n",
        "      ff94.append(f94)\n",
        "      ff95.append(f95)\n",
        "      ff96.append(f96)\n",
        "      ff97.append(f97)\n",
        "\n",
        "data2save = pd.DataFrame({'ff1':ff1,'ff2':ff2,'ff3':ff3,'ff4':ff4,'ff5':ff5,'ff6':ff6,'ff7':ff7,'ff8':ff8,'ff9':ff9,'ff10':ff10,\n",
        "'ff11':ff11,'ff12':ff12,'ff13':ff13,'ff14':ff14,'ff15':ff15,'ff16':ff16,'ff17':ff17,'ff18':ff18,'ff19':ff19,'ff20':ff20,\n",
        "'ff21':ff21,'ff22':ff22,'ff23':ff23,'ff24':ff24,'ff25':ff25,'ff26':ff26,'ff27':ff27,'ff28':ff28,'ff29':ff29,'ff30':ff30,\n",
        "'ff31':ff31,'ff32':ff32,'ff33':ff33,'ff34':ff34,'ff35':ff35,'ff36':ff36,'ff37':ff37,'ff38':ff38,'ff39':ff39,'ff40':ff40,\n",
        "'ff41':ff41,'ff42':ff42,'ff43':ff43,'ff44':ff44,'ff45':ff45,'ff46':ff46,'ff47':ff47,'ff48':ff48,'ff49':ff49,'ff50':ff50,\n",
        "'ff51':ff51,'ff52':ff52,'ff53':ff53,'ff54':ff54,'ff55':ff55,'ff56':ff56,'ff57':ff57,'ff58':ff58,'ff59':ff59,'ff60':ff60,\n",
        "'ff61':ff61,'ff62':ff62,'ff63':ff63,'ff64':ff64,'ff65':ff65,'ff66':ff66,'ff67':ff67,'ff68':ff68,'ff69':ff69,'ff70':ff70,\n",
        "'ff71':ff71,'ff72':ff72,'ff73':ff73,'ff74':ff74,'ff75':ff75,'ff76':ff76,'ff77':ff77,'ff78':ff78,'ff79':ff79,'ff80':ff80,\n",
        "'ff81':ff81,'ff82':ff82,'ff83':ff83,'ff84':ff84,'ff85':ff85,'ff86':ff86,'ff87':ff87,'ff88':ff88,'ff89':ff89,'ff90':ff90,\n",
        "'ff91':ff91,'ff92':ff92,'ff93':ff93,'ff94':ff94,'ff95':ff95,'ff96':ff96,'ff97':ff97})\n",
        "\n",
        "\n",
        "data2save.to_csv(\"Input_Dataset.csv\",index=False,header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImuCfPTb-bis"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data2ML = pd.read_csv(\"Input_Dataset.csv\")\n",
        "\n",
        "# X = data2ML.drop('ff97', axis=1)\n",
        "# y = data2ML['ff97']\n",
        "\n",
        "\n",
        "y_t = np.array(data2ML['ff97'])\n",
        "X_t = data2ML\n",
        "X_t = data2ML.drop(['ff97'],axis=1)\n",
        "X_t = np.array(X_t)\n",
        "\n",
        "print(\"shape of Y :\"+str(y_t.shape))\n",
        "print(\"shape of X :\"+str(X_t.shape))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9N5OYwhFW-5"
      },
      "outputs": [],
      "source": [
        "## SVM\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_t = scaler.fit_transform(X_t)\n",
        "\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(X_t,y_t,test_size=.20,random_state=42)\n",
        "print(\"shape of X Train :\"+str(X_train.shape))\n",
        "print(\"shape of X Test :\"+str(X_test.shape))\n",
        "print(\"shape of Y Train :\"+str(Y_train.shape))\n",
        "print(\"shape of Y Test :\"+str(Y_test.shape))\n",
        "\n",
        "\n",
        "clf = SVC(kernel='linear',C=1).fit(X_train,Y_train)\n",
        "scoretrain = clf.score(X_train,Y_train)\n",
        "scoretest  = clf.score(X_test,Y_test)\n",
        "print(\"training score :{:2f} , Test Score: {:2f} \\n\".format(scoretrain,scoretest))\n",
        "\n",
        "from sklearn import metrics\n",
        "y_predicted = clf.predict(X_test)\n",
        "confusion_matrix = metrics.confusion_matrix(Y_test, y_predicted)\n",
        "\n",
        "Accuracy = metrics.accuracy_score(Y_test, y_predicted)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion = confusion_matrix(Y_test, y_predicted)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(Y_test, y_predicted)*100))\n",
        "\n",
        "print('Precision: {:.2f}'.format(precision_score(Y_test, y_predicted, average='weighted')*100))\n",
        "print('Recall: {:.2f}'.format(recall_score(Y_test, y_predicted, average='weighted')*100))\n",
        "print('F1-score: {:.2f}'.format(f1_score(Y_test, y_predicted, average='weighted')*100))\n",
        "\n",
        "TP_class1 = confusion[2][2]\n",
        "TN_class1 = confusion[0][2]+confusion[1][2]\n",
        "print()\n",
        "precision_class1 = round(TP_class1/(TP_class1+TN_class1)*100,2)\n",
        "print('Percentage of positive predictions (class1) : ',precision_class1)\n",
        "\n",
        "print()\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGnGWQs_Qi6I"
      },
      "outputs": [],
      "source": [
        "# Stochastic Gradient Descent Classifier\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "SGDC = SGDClassifier()\n",
        "SGDC.fit(X_train, Y_train)\n",
        "print(SGDC.score(X_test, Y_test))\n",
        "\n",
        "y_predicted = SGDC.predict(X_test)\n",
        "confusion_matrix = metrics.confusion_matrix(Y_test, y_predicted)\n",
        "\n",
        "Accuracy = metrics.accuracy_score(Y_test, y_predicted)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion = confusion_matrix(Y_test, y_predicted)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(Y_test, y_predicted)*100))\n",
        "\n",
        "print('Precision: {:.2f}'.format(precision_score(Y_test, y_predicted, average='weighted')*100))\n",
        "print('Recall: {:.2f}'.format(recall_score(Y_test, y_predicted, average='weighted')*100))\n",
        "print('F1-score: {:.2f}'.format(f1_score(Y_test, y_predicted, average='weighted')*100))\n",
        "\n",
        "TP_class1 = confusion[2][2]\n",
        "TN_class1 = confusion[0][2]+confusion[1][2]\n",
        "print()\n",
        "precision_class1 = round(TP_class1/(TP_class1+TN_class1)*100,2)\n",
        "print('Percentage of positive predictions (class1) : ',precision_class1)\n",
        "\n",
        "print()\n",
        "print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRd6jiYOR1yY"
      },
      "outputs": [],
      "source": [
        "# Decision Tree\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "DT = DecisionTreeClassifier()\n",
        "DT.fit(X_train, Y_train)\n",
        "print(DT.score(X_test, Y_test))\n",
        "\n",
        "y_predicted = DT.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "# y_predicted = clf.predict(X_test)\n",
        "confusion_matrix = metrics.confusion_matrix(Y_test, y_predicted)\n",
        "\n",
        "Accuracy = metrics.accuracy_score(Y_test, y_predicted)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion = confusion_matrix(Y_test, y_predicted)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(Y_test, y_predicted)*100))\n",
        "\n",
        "print('Precision: {:.2f}'.format(precision_score(Y_test, y_predicted, average='weighted')*100))\n",
        "print('Recall: {:.2f}'.format(recall_score(Y_test, y_predicted, average='weighted')*100))\n",
        "print('F1-score: {:.2f}'.format(f1_score(Y_test, y_predicted, average='weighted')*100))\n",
        "\n",
        "TP_class1 = confusion[2][2]\n",
        "TN_class1 = confusion[0][2]+confusion[1][2]\n",
        "print()\n",
        "precision_class1 = round(TP_class1/(TP_class1+TN_class1)*100,2)\n",
        "print('Percentage of positive predictions (class1) : ',precision_class1)\n",
        "\n",
        "print()\n",
        "print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzOk4bOBTdYh"
      },
      "outputs": [],
      "source": [
        "# Gaussian Naive Bayes\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "GNB = GaussianNB()\n",
        "GNB.fit(X_train, Y_train)\n",
        "print(GNB.score(X_test, Y_test))\n",
        "\n",
        "y_predicted = GNB.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "# y_predicted = clf.predict(X_test)\n",
        "confusion_matrix = metrics.confusion_matrix(Y_test, y_predicted)\n",
        "\n",
        "Accuracy = metrics.accuracy_score(Y_test, y_predicted)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion = confusion_matrix(Y_test, y_predicted)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(Y_test, y_predicted)*100))\n",
        "\n",
        "print('Precision: {:.2f}'.format(precision_score(Y_test, y_predicted, average='weighted')*100))\n",
        "print('Recall: {:.2f}'.format(recall_score(Y_test, y_predicted, average='weighted')*100))\n",
        "print('F1-score: {:.2f}'.format(f1_score(Y_test, y_predicted, average='weighted')*100))\n",
        "\n",
        "TP_class1 = confusion[2][2]\n",
        "TN_class1 = confusion[0][2]+confusion[1][2]\n",
        "print()\n",
        "precision_class1 = round(TP_class1/(TP_class1+TN_class1)*100,2)\n",
        "print('Percentage of positive predictions (class1) : ',precision_class1)\n",
        "\n",
        "print()\n",
        "print()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63RA_IkguZxb"
      },
      "outputs": [],
      "source": [
        "# Multinomial Naive Bayes\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "MNB = MultinomialNB()\n",
        "MNB.fit(X_train, Y_train)\n",
        "print(MNB.score(X_test, Y_test))\n",
        "\n",
        "y_predicted = MNB.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "# y_predicted = clf.predict(X_test)\n",
        "confusion_matrix = metrics.confusion_matrix(Y_test, y_predicted)\n",
        "\n",
        "Accuracy = metrics.accuracy_score(Y_test, y_predicted)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion = confusion_matrix(Y_test, y_predicted)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(Y_test, y_predicted)*100))\n",
        "\n",
        "print('Precision: {:.2f}'.format(precision_score(Y_test, y_predicted, average='weighted')*100))\n",
        "print('Recall: {:.2f}'.format(recall_score(Y_test, y_predicted, average='weighted')*100))\n",
        "print('F1-score: {:.2f}'.format(f1_score(Y_test, y_predicted, average='weighted')*100))\n",
        "\n",
        "TP_class1 = confusion[2][2]\n",
        "TN_class1 = confusion[0][2]+confusion[1][2]\n",
        "print()\n",
        "precision_class1 = round(TP_class1/(TP_class1+TN_class1)*100,2)\n",
        "print('Percentage of positive predictions (class1) : ',precision_class1)\n",
        "\n",
        "print()\n",
        "print()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQl8w6hGuoXi"
      },
      "outputs": [],
      "source": [
        "# Complement Naive Bayes\n",
        "\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "\n",
        "CNB = ComplementNB()\n",
        "CNB.fit(X_train, Y_train)\n",
        "print(CNB.score(X_test, Y_test))\n",
        "\n",
        "y_predicted = CNB.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "# y_predicted = clf.predict(X_test)\n",
        "confusion_matrix = metrics.confusion_matrix(Y_test, y_predicted)\n",
        "\n",
        "Accuracy = metrics.accuracy_score(Y_test, y_predicted)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion = confusion_matrix(Y_test, y_predicted)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(Y_test, y_predicted)*100))\n",
        "\n",
        "print('Precision: {:.2f}'.format(precision_score(Y_test, y_predicted, average='weighted')*100))\n",
        "print('Recall: {:.2f}'.format(recall_score(Y_test, y_predicted, average='weighted')*100))\n",
        "print('F1-score: {:.2f}'.format(f1_score(Y_test, y_predicted, average='weighted')*100))\n",
        "\n",
        "TP_class1 = confusion[2][2]\n",
        "TN_class1 = confusion[0][2]+confusion[1][2]\n",
        "print()\n",
        "precision_class1 = round(TP_class1/(TP_class1+TN_class1)*100,2)\n",
        "print('Percentage of positive predictions (class1) : ',precision_class1)\n",
        "\n",
        "print()\n",
        "print()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEbjJmIguzeu"
      },
      "outputs": [],
      "source": [
        "# Bernoulli Naive Bayes\n",
        "\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "BNB = BernoulliNB()\n",
        "BNB.fit(X_train, Y_train)\n",
        "print(BNB.score(X_test, Y_test))\n",
        "\n",
        "y_predicted = BNB.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "# y_predicted = clf.predict(X_test)\n",
        "confusion_matrix = metrics.confusion_matrix(Y_test, y_predicted)\n",
        "\n",
        "Accuracy = metrics.accuracy_score(Y_test, y_predicted)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion = confusion_matrix(Y_test, y_predicted)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(Y_test, y_predicted)*100))\n",
        "\n",
        "print('Precision: {:.2f}'.format(precision_score(Y_test, y_predicted, average='weighted')*100))\n",
        "print('Recall: {:.2f}'.format(recall_score(Y_test, y_predicted, average='weighted')*100))\n",
        "print('F1-score: {:.2f}'.format(f1_score(Y_test, y_predicted, average='weighted')*100))\n",
        "\n",
        "TP_class1 = confusion[2][2]\n",
        "TN_class1 = confusion[0][2]+confusion[1][2]\n",
        "print()\n",
        "precision_class1 = round(TP_class1/(TP_class1+TN_class1)*100,2)\n",
        "print('Percentage of positive predictions (class1) : ',precision_class1)\n",
        "\n",
        "print()\n",
        "print()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0xRcWVFu-VD"
      },
      "outputs": [],
      "source": [
        "# k-nearest neighbors\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "KNN = KNeighborsClassifier(n_neighbors=3)\n",
        "KNN.fit(X_train, Y_train)\n",
        "print(KNN.score(X_test, Y_test))\n",
        "\n",
        "y_predicted = KNN.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "# y_predicted = clf.predict(X_test)\n",
        "confusion_matrix = metrics.confusion_matrix(Y_test, y_predicted)\n",
        "\n",
        "Accuracy = metrics.accuracy_score(Y_test, y_predicted)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion = confusion_matrix(Y_test, y_predicted)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(Y_test, y_predicted)*100))\n",
        "\n",
        "print('Precision: {:.2f}'.format(precision_score(Y_test, y_predicted, average='weighted')*100))\n",
        "print('Recall: {:.2f}'.format(recall_score(Y_test, y_predicted, average='weighted')*100))\n",
        "print('F1-score: {:.2f}'.format(f1_score(Y_test, y_predicted, average='weighted')*100))\n",
        "\n",
        "TP_class1 = confusion[2][2]\n",
        "TN_class1 = confusion[0][2]+confusion[1][2]\n",
        "print()\n",
        "precision_class1 = round(TP_class1/(TP_class1+TN_class1)*100,2)\n",
        "print('Percentage of positive predictions (class1) : ',precision_class1)\n",
        "\n",
        "print()\n",
        "print()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZ-hwF9Zy_4a"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logR = LogisticRegression(random_state=0).fit(X_train, Y_train)\n",
        "print(logR.score(X_test, Y_test))\n",
        "\n",
        "y_predicted = logR.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "# y_predicted = clf.predict(X_test)\n",
        "confusion_matrix = metrics.confusion_matrix(Y_test, y_predicted)\n",
        "\n",
        "Accuracy = metrics.accuracy_score(Y_test, y_predicted)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion = confusion_matrix(Y_test, y_predicted)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(Y_test, y_predicted)*100))\n",
        "\n",
        "print('Precision: {:.2f}'.format(precision_score(Y_test, y_predicted, average='weighted')*100))\n",
        "print('Recall: {:.2f}'.format(recall_score(Y_test, y_predicted, average='weighted')*100))\n",
        "print('F1-score: {:.2f}'.format(f1_score(Y_test, y_predicted, average='weighted')*100))\n",
        "\n",
        "TP_class1 = confusion[2][2]\n",
        "TN_class1 = confusion[0][2]+confusion[1][2]\n",
        "print()\n",
        "precision_class1 = round(TP_class1/(TP_class1+TN_class1)*100,2)\n",
        "print('Percentage of positive predictions (class1) : ',precision_class1)\n",
        "\n",
        "print()\n",
        "print()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFUKL9QszjBX"
      },
      "outputs": [],
      "source": [
        "# Gradient Boosting Classifier\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "\n",
        "GBC = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0).fit(X_train, Y_train)\n",
        "print(GBC.score(X_test, Y_test))\n",
        "\n",
        "y_predicted = GBC.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "# y_predicted = clf.predict(X_test)\n",
        "confusion_matrix = metrics.confusion_matrix(Y_test, y_predicted)\n",
        "\n",
        "Accuracy = metrics.accuracy_score(Y_test, y_predicted)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion = confusion_matrix(Y_test, y_predicted)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(Y_test, y_predicted)*100))\n",
        "\n",
        "print('Precision: {:.2f}'.format(precision_score(Y_test, y_predicted, average='weighted')*100))\n",
        "print('Recall: {:.2f}'.format(recall_score(Y_test, y_predicted, average='weighted')*100))\n",
        "print('F1-score: {:.2f}'.format(f1_score(Y_test, y_predicted, average='weighted')*100))\n",
        "\n",
        "TP_class1 = confusion[2][2]\n",
        "TN_class1 = confusion[0][2]+confusion[1][2]\n",
        "print()\n",
        "precision_class1 = round(TP_class1/(TP_class1+TN_class1)*100,2)\n",
        "print('Percentage of positive predictions (class1) : ',precision_class1)\n",
        "\n",
        "print()\n",
        "print()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvpLg6EtCsKT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMZ1wyyI1UuJ"
      },
      "outputs": [],
      "source": [
        "# Ensembling (voting)\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "Vmodel = VotingClassifier(estimators=[('clf',clf),('GNB',GNB),('MNB',MNB),('CNB',CNB),('BNB',BNB),('KNN',KNN),('logR',logR),('GBC',GBC),('SGDC',SGDC),('DT',DT)])\n",
        "Vmodel.fit(X_train, Y_train)\n",
        "\n",
        "print(Vmodel.score(X_train, Y_train))\n",
        "\n",
        "y_predicted = Vmodel.predict(X_train)\n",
        "\n",
        "confusion_matrix = metrics.confusion_matrix(Y_train, y_predicted)\n",
        "\n",
        "Accuracy = metrics.accuracy_score(Y_train, y_predicted)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion = confusion_matrix(Y_train, y_predicted)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(Y_train, y_predicted)*100))\n",
        "\n",
        "print('Precision: {:.2f}'.format(precision_score(Y_train, y_predicted, average='weighted')*100))\n",
        "print('Recall: {:.2f}'.format(recall_score(Y_train, y_predicted, average='weighted')*100))\n",
        "print('F1-score: {:.2f}'.format(f1_score(Y_train, y_predicted, average='weighted')*100))\n",
        "\n",
        "TP_class1 = confusion[2][2]\n",
        "TN_class1 = confusion[0][2]+confusion[1][2]\n",
        "print()\n",
        "precision_class1 = round(TP_class1/(TP_class1+TN_class1)*100,2)\n",
        "print('Percentage of positive predictions (class1) : ',precision_class1)\n",
        "\n",
        "print()\n",
        "print()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsEm767RGHLy"
      },
      "outputs": [],
      "source": [
        "# AdaBoosting\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "AdaBoosting = AdaBoostClassifier().fit(X_train, Y_train)\n",
        "\n",
        "y_predicted = AdaBoosting.predict(X_test)\n",
        "\n",
        "# y_predicted = clf.predict(X_test)\n",
        "confusion_matrix = metrics.confusion_matrix(Y_test, y_predicted)\n",
        "\n",
        "Accuracy = metrics.accuracy_score(Y_test, y_predicted)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion = confusion_matrix(Y_test, y_predicted)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(Y_test, y_predicted)*100))\n",
        "\n",
        "print('Precision: {:.2f}'.format(precision_score(Y_test, y_predicted, average='weighted')*100))\n",
        "print('Recall: {:.2f}'.format(recall_score(Y_test, y_predicted, average='weighted')*100))\n",
        "print('F1-score: {:.2f}'.format(f1_score(Y_test, y_predicted, average='weighted')*100))\n",
        "\n",
        "TP_class1 = confusion[2][2]\n",
        "TN_class1 = confusion[0][2]+confusion[1][2]\n",
        "print()\n",
        "precision_class1 = round(TP_class1/(TP_class1+TN_class1)*100,2)\n",
        "print('Percentage of positive predictions (class1) : ',precision_class1)\n",
        "\n",
        "print()\n",
        "print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgU6YdMxI-jn"
      },
      "outputs": [],
      "source": [
        "# blending ensemble for classification\n",
        "from numpy import hstack\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# get a list of base models\n",
        "def get_models():\n",
        "\tmodels = list()\n",
        "\tmodels.append(('clf',clf))\n",
        "\tmodels.append(('GNB',GNB))\n",
        "\tmodels.append(('MNB',MNB))\n",
        "\tmodels.append(('BNB',BNB))\n",
        "\tmodels.append(('KNN',KNN))\n",
        "\tmodels.append(('logR',logR))\n",
        "\tmodels.append(('GBC',GBC))\n",
        "\tmodels.append(('SGDC',SGDC))\n",
        "\tmodels.append(('DT',DT))\n",
        "\treturn models\n",
        "\n",
        "\n",
        "# fit the blending ensemble\n",
        "def fit_ensemble(models, X_train, X_val, y_train, y_val):\n",
        "\t# fit all models on the training set and predict on hold out set\n",
        "\tmeta_X = list()\n",
        "\tfor name, model in models:\n",
        "\t\tmodel.fit(X_train, y_train)\n",
        "\t\t# predict on hold out set\n",
        "\t\tyhat = model.predict(X_val)\n",
        "\t\t# reshape predictions into a matrix with one column\n",
        "\t\tyhat = yhat.reshape(len(yhat), 1)\n",
        "\t\t# store predictions as input for blending\n",
        "\t\tmeta_X.append(yhat)\n",
        "\t# create 2d array from predictions, each set is an input feature\n",
        "\tmeta_X = hstack(meta_X)\n",
        "\t# define blending model\n",
        "\tblender = LogisticRegression()\n",
        "\t# fit on predictions from base models\n",
        "\tblender.fit(meta_X, y_val)\n",
        "\treturn blender\n",
        "\n",
        "# make a prediction with the blending ensemble\n",
        "def predict_ensemble(models, blender, X_test):\n",
        "\t# make predictions with base models\n",
        "\tmeta_X = list()\n",
        "\tfor name, model in models:\n",
        "\t\t# predict with base model\n",
        "\t\tyhat = model.predict(X_test)\n",
        "\t\t# reshape predictions into a matrix with one column\n",
        "\t\tyhat = yhat.reshape(len(yhat), 1)\n",
        "\t\t# store prediction\n",
        "\t\tmeta_X.append(yhat)\n",
        "\t# create 2d array from predictions, each set is an input feature\n",
        "\tmeta_X = hstack(meta_X)\n",
        "\t# predict\n",
        "\treturn blender.predict(meta_X)\n",
        "\n",
        "models = get_models()\n",
        "blender = fit_ensemble(models, X_train, X_test, Y_train, Y_test)\n",
        "y_predicted = predict_ensemble(models, blender, X_test)\n",
        "\n",
        "confusion_matrix = metrics.confusion_matrix(Y_test, y_predicted)\n",
        "\n",
        "Accuracy = metrics.accuracy_score(Y_test, y_predicted)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion = confusion_matrix(Y_test, y_predicted)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(Y_test, y_predicted)*100))\n",
        "\n",
        "print('Precision: {:.2f}'.format(precision_score(Y_test, y_predicted, average='weighted')*100))\n",
        "print('Recall: {:.2f}'.format(recall_score(Y_test, y_predicted, average='weighted')*100))\n",
        "print('F1-score: {:.2f}'.format(f1_score(Y_test, y_predicted, average='weighted')*100))\n",
        "\n",
        "TP_class1 = confusion[2][2]\n",
        "TN_class1 = confusion[0][2]+confusion[1][2]\n",
        "print()\n",
        "precision_class1 = round(TP_class1/(TP_class1+TN_class1)*100,2)\n",
        "print('Percentage of positive predictions (class1) : ',precision_class1)\n",
        "\n",
        "print()\n",
        "print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZnqYzmjeC-k"
      },
      "outputs": [],
      "source": [
        "# AdaBoosting\n",
        "\n",
        "print(\"The evaluation parameters of the AdaBoosting is on higher side.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrBRBuPieDE2"
      },
      "outputs": [],
      "source": [
        "# Testing\n",
        "\n",
        "with open(\"/mydrive/soham/testing_set1.pkl\", \"rb\") as f:\n",
        "    df = pickle.load(f)\n",
        "\n",
        "data = []\n",
        "number_of_instances = 2\n",
        "\n",
        "for i in range(number_of_instances):\n",
        "  data.append(df[i])\n",
        "\n",
        "\n",
        "# data = pd.read_csv(file)\n",
        "# increase = 0\n",
        "# decrease = 1\n",
        "# no big change = 2\n",
        "\n",
        "for ii in range(number_of_instances):\n",
        "  Close = data[ii]['Close']\n",
        "  Open = data[ii]['Open']\n",
        "  High = data[ii]['High']\n",
        "  Low = data[ii]['Low']\n",
        "  Volume = data[ii]['Volume']\n",
        "\n",
        "\n",
        "  Change = [0]* len(Close)\n",
        "  Momentum = [0]* len(Close)\n",
        "  Momentum[0] = 2\n",
        "\n",
        "  for i in range (1,len(Close)):\n",
        "      if Close[i] - Close[i-1] > 0.005:\n",
        "          Momentum[i] = 0\n",
        "          Change[i] = (Close[i]-Close[i-1])/Close[i-1]\n",
        "      elif Close[i-1] - Close[i] > 0.005 :\n",
        "          Momentum[i] = 1\n",
        "          Change[i] = (Close[i-1] - Close[i]) / Close[i - 1]\n",
        "      else:\n",
        "          if Close[i] > Close[i-1] :\n",
        "              Momentum[i] = 2\n",
        "              Change[i] = (Close[i]-Close[i-1])/Close[i-1]\n",
        "          else :\n",
        "              Momentum[i] = 2\n",
        "              Change[i] = (Close[i-1] - Close[i]) / Close[i - 1]\n",
        "\n",
        "  xl = pd.DataFrame({'Open':Open,'High':High,'Low':Low,'Volume':Volume,'Close':Close,'Change':Change,'Momentum':Momentum})\n",
        "  xl.to_csv(\"DATA\" + str(ii)  + \".csv\",index=False,header=True)\n",
        "\n",
        "data2extract_features = []\n",
        "for ii in range(number_of_instances):\n",
        "    data2extract_features.append(pd.read_csv(\"DATA\" + str(ii)  + \".csv\"))\n",
        "\n",
        "ff1,ff2,ff3,ff4,ff5,ff6,ff7,ff8,ff9,ff10 = [],[],[],[],[],[],[],[],[],[]\n",
        "ff11,ff12,ff13,ff14,ff15,ff16,ff17,ff18,ff19,ff20 = [],[],[],[],[],[],[],[],[],[]\n",
        "ff21,ff22,ff23,ff24,ff25,ff26,ff27,ff28,ff29,ff30 = [],[],[],[],[],[],[],[],[],[]\n",
        "ff31,ff32,ff33,ff34,ff35,ff36,ff37,ff38,ff39,ff40 = [],[],[],[],[],[],[],[],[],[]\n",
        "ff41,ff42,ff43,ff44,ff45,ff46,ff47,ff48,ff49,ff50 = [],[],[],[],[],[],[],[],[],[]\n",
        "ff51,ff52,ff53,ff54,ff55,ff56,ff57,ff58,ff59,ff60 = [],[],[],[],[],[],[],[],[],[]\n",
        "ff61,ff62,ff63,ff64,ff65,ff66,ff67,ff68,ff69,ff70 = [],[],[],[],[],[],[],[],[],[]\n",
        "ff71,ff72,ff73,ff74,ff75,ff76,ff77,ff78,ff79,ff80 = [],[],[],[],[],[],[],[],[],[]\n",
        "ff81,ff82,ff83,ff84,ff85,ff86,ff87,ff88,ff89,ff90 = [],[],[],[],[],[],[],[],[],[]\n",
        "ff91,ff92,ff93,ff94,ff95,ff96,ff97 = [],[],[],[],[],[],[]\n",
        "\n",
        "for ii in range(number_of_instances):\n",
        "  # for i in range(5,15):\n",
        "  for i in range(5,len(data2extract_features[0])):\n",
        "      f1 = (data2extract_features[ii]['Close'][i] + data2extract_features[ii]['Close'][i-1] + data2extract_features[ii]['Close'][i-2] + data2extract_features[ii]['Close'][i-3] + data2extract_features[ii]['Close'][i-4])/5\n",
        "      f2 = (data2extract_features[ii]['Open'][i] + data2extract_features[ii]['Open'][i-1] + data2extract_features[ii]['Open'][i-2] + data2extract_features[ii]['Open'][i-3] + data2extract_features[ii]['Open'][i-4])/5\n",
        "      f3 = (data2extract_features[ii]['High'][i] + data2extract_features[ii]['High'][i-1] + data2extract_features[ii]['High'][i-2] + data2extract_features[ii]['High'][i-3] + data2extract_features[ii]['High'][i-4])/5\n",
        "      f4 = (data2extract_features[ii]['Low'][i] + data2extract_features[ii]['Low'][i-1] + data2extract_features[ii]['Low'][i-2] + data2extract_features[ii]['Low'][i-3] + data2extract_features[ii]['Low'][i-4])/5\n",
        "      f5 = (data2extract_features[ii]['Volume'][i] + data2extract_features[ii]['Volume'][i-1] + data2extract_features[ii]['Volume'][i-2] + data2extract_features[ii]['Volume'][i-3] + data2extract_features[ii]['Volume'][i-4])/5\n",
        "      f6 = (data2extract_features[ii]['Change'][i] + data2extract_features[ii]['Change'][i-1] + data2extract_features[ii]['Change'][i-2] + data2extract_features[ii]['Change'][i-3] + data2extract_features[ii]['Change'][i-4])/5\n",
        "\n",
        "\n",
        "      # close_array = [data2extract_features[ii]['Close'][i] + data2extract_features[ii]['Close'][i-1] + data2extract_features[ii]['Close'][i-2] + data2extract_features[ii]['Close'][i-3] + data2extract_features[ii]['Close'][i-4]]\n",
        "      # open_array = [data2extract_features[ii]['Open'][i] + data2extract_features[ii]['Open'][i-1] + data2extract_features[ii]['Open'][i-2] + data2extract_features[ii]['Open'][i-3] + data2extract_features[ii]['Open'][i-4]]\n",
        "      # high_array = [data2extract_features[ii]['High'][i] + data2extract_features[ii]['High'][i-1] + data2extract_features[ii]['High'][i-2] + data2extract_features[ii]['High'][i-3] + data2extract_features[ii]['High'][i-4]]\n",
        "      # low_array = [data2extract_features[ii]['Low'][i] + data2extract_features[ii]['Low'][i-1] + data2extract_features[ii]['Low'][i-2] + data2extract_features[ii]['Low'][i-3] + data2extract_features[ii]['Low'][i-4]]\n",
        "      # volume_array = [data2extract_features[ii]['Volume'][i] + data2extract_features[ii]['Volume'][i-1] + data2extract_features[ii]['Volume'][i-2] + data2extract_features[ii]['Volume'][i-3] + data2extract_features[ii]['Volume'][i-4]]\n",
        "      # change_array = [data2extract_features[ii]['Change'][i] + data2extract_features[ii]['Change'][i-1] + data2extract_features[ii]['Change'][i-2] + data2extract_features[ii]['Change'][i-3] + data2extract_features[ii]['Change'][i-4]]\n",
        "\n",
        "      close_array = [data2extract_features[ii]['Close'][i] , data2extract_features[ii]['Close'][i-1] , data2extract_features[ii]['Close'][i-2] , data2extract_features[ii]['Close'][i-3] , data2extract_features[ii]['Close'][i-4]]\n",
        "      open_array = [data2extract_features[ii]['Open'][i] , data2extract_features[ii]['Open'][i-1] , data2extract_features[ii]['Open'][i-2] , data2extract_features[ii]['Open'][i-3] , data2extract_features[ii]['Open'][i-4]]\n",
        "      high_array = [data2extract_features[ii]['High'][i] , data2extract_features[ii]['High'][i-1] , data2extract_features[ii]['High'][i-2] , data2extract_features[ii]['High'][i-3] , data2extract_features[ii]['High'][i-4]]\n",
        "      low_array = [data2extract_features[ii]['Low'][i] , data2extract_features[ii]['Low'][i-1] , data2extract_features[ii]['Low'][i-2] , data2extract_features[ii]['Low'][i-3] , data2extract_features[ii]['Low'][i-4]]\n",
        "      volume_array = [data2extract_features[ii]['Volume'][i] , data2extract_features[ii]['Volume'][i-1] , data2extract_features[ii]['Volume'][i-2] , data2extract_features[ii]['Volume'][i-3] , data2extract_features[ii]['Volume'][i-4]]\n",
        "      change_array = [data2extract_features[ii]['Change'][i] , data2extract_features[ii]['Change'][i-1] , data2extract_features[ii]['Change'][i-2] , data2extract_features[ii]['Change'][i-3] , data2extract_features[ii]['Change'][i-4]]\n",
        "\n",
        "      f7 = statistics.median(close_array)\n",
        "      f8 = statistics.harmonic_mean(close_array)\n",
        "      f9 = statistics.median_low(close_array)\n",
        "      f10 = statistics.median_high(close_array)\n",
        "      f11 = statistics.median_grouped(close_array)\n",
        "      f12 = statistics.mode(close_array)\n",
        "      f13 = statistics.pstdev(close_array)\n",
        "      f14 = statistics.pvariance(close_array)\n",
        "      k = stats.describe(close_array)\n",
        "      f15 = k[1][0]\n",
        "      f16 = k[1][1]\n",
        "      f17 = k[2]\n",
        "      f18 = k[3]\n",
        "      if math.isnan(f18):\n",
        "        f18 = 0\n",
        "      f19 = k[4]\n",
        "      if math.isnan(f19):\n",
        "        f19 = 0\n",
        "      f20 = k[5]\n",
        "      if math.isnan(f20):\n",
        "        f20 = 0\n",
        "      f21 = k[2]\n",
        "\n",
        "#\n",
        "      f22 = statistics.median(open_array)\n",
        "      f23 = statistics.harmonic_mean(open_array)\n",
        "      f24 = statistics.median_low(open_array)\n",
        "      f25 = statistics.median_high(open_array)\n",
        "      f26 = statistics.median_grouped(open_array)\n",
        "      f27 = statistics.mode(open_array)\n",
        "      f28 = statistics.pstdev(open_array)\n",
        "      f29 = statistics.pvariance(open_array)\n",
        "      k = stats.describe(open_array)\n",
        "      f30 = k[1][0]\n",
        "      f31 = k[1][1]\n",
        "      f32 = k[2]\n",
        "      f33 = k[3]\n",
        "      if math.isnan(f33):\n",
        "        f33 = 0\n",
        "      f34 = k[4]\n",
        "      if math.isnan(f34):\n",
        "        f34 = 0\n",
        "      f35 = k[5]\n",
        "      if math.isnan(f35):\n",
        "        f35 = 0\n",
        "      f36 = k[2]\n",
        "#\n",
        "      f37 = statistics.median(high_array)\n",
        "      f38 = statistics.harmonic_mean(high_array)\n",
        "      f39 = statistics.median_low(high_array)\n",
        "      f40 = statistics.median_high(high_array)\n",
        "      f41 = statistics.median_grouped(high_array)\n",
        "      f42 = statistics.mode(high_array)\n",
        "      f43 = statistics.pstdev(high_array)\n",
        "      f44 = statistics.pvariance(high_array)\n",
        "      k = stats.describe(high_array)\n",
        "      f45 = k[1][0]\n",
        "      f46 = k[1][1]\n",
        "      f47 = k[2]\n",
        "      f48 = k[3]\n",
        "      if math.isnan(f48):\n",
        "        f48 = 0\n",
        "      f49 = k[4]\n",
        "      if math.isnan(f49):\n",
        "        f49 = 0\n",
        "      f50 = k[5]\n",
        "      if math.isnan(f50):\n",
        "        f50 = 0\n",
        "      f51 = k[2]\n",
        "\n",
        "\n",
        "#\n",
        "      f52 = statistics.median(low_array)\n",
        "      f53 = statistics.harmonic_mean(low_array)\n",
        "      f54 = statistics.median_low(low_array)\n",
        "      f55 = statistics.median_high(low_array)\n",
        "      f56 = statistics.median_grouped(low_array)\n",
        "      f57 = statistics.mode(low_array)\n",
        "      f58 = statistics.pstdev(low_array)\n",
        "      f59 = statistics.pvariance(low_array)\n",
        "      k = stats.describe(low_array)\n",
        "      f60 = k[1][0]\n",
        "      f61 = k[1][1]\n",
        "      f62 = k[2]\n",
        "      f63 = k[3]\n",
        "      if math.isnan(f63):\n",
        "        f63 = 0\n",
        "      f64 = k[4]\n",
        "      if math.isnan(f64):\n",
        "        f64 = 0\n",
        "      f65 = k[5]\n",
        "      if math.isnan(f65):\n",
        "        f65 = 0\n",
        "      f66 = k[2]\n",
        "\n",
        "#\n",
        "      f67 = statistics.median(volume_array)\n",
        "      f68 = statistics.harmonic_mean(volume_array)\n",
        "      f69 = statistics.median_low(volume_array)\n",
        "      f70 = statistics.median_high(volume_array)\n",
        "      f71 = statistics.median_grouped(volume_array)\n",
        "      f72 = statistics.mode(volume_array)\n",
        "      f73 = statistics.pstdev(volume_array)\n",
        "      f74 = statistics.pvariance(volume_array)\n",
        "      k = stats.describe(volume_array)\n",
        "      f75 = k[1][0]\n",
        "      f76 = k[1][1]\n",
        "      f77 = k[2]\n",
        "      f78 = k[3]\n",
        "      if math.isnan(f78):\n",
        "        f78 = 0\n",
        "      f79 = k[4]\n",
        "      if math.isnan(f79):\n",
        "        f79 = 0\n",
        "      f80 = k[5]\n",
        "      if math.isnan(f80):\n",
        "        f80 = 0\n",
        "      f81 = k[2]\n",
        "\n",
        "#\n",
        "      f82 = statistics.median(change_array)\n",
        "      f83 = statistics.harmonic_mean(change_array)\n",
        "      f84 = statistics.median_low(change_array)\n",
        "      f85 = statistics.median_high(change_array)\n",
        "      f86 = statistics.median_grouped(change_array)\n",
        "      f87 = statistics.mode(change_array)\n",
        "      f88 = statistics.pstdev(change_array)\n",
        "      f89 = statistics.pvariance(change_array)\n",
        "      k = stats.describe(change_array)\n",
        "      f90 = k[1][0]\n",
        "      f91 = k[1][1]\n",
        "      f92 = k[2]\n",
        "      f93 = k[3]\n",
        "      if math.isnan(f93):\n",
        "        f93 = 0\n",
        "      f94 = k[4]\n",
        "      if math.isnan(f94):\n",
        "        f94 = 0\n",
        "      f95 = k[5]\n",
        "      if math.isnan(f95):\n",
        "        f95 = 0\n",
        "      f96 = k[2]\n",
        "\n",
        "      f97 = data2extract_features[ii]['Momentum'][i-4]\n",
        "\n",
        "      ff1.append(f1)\n",
        "      ff2.append(f2)\n",
        "      ff3.append(f3)\n",
        "      ff4.append(f4)\n",
        "      ff5.append(f5)\n",
        "      ff6.append(f6)\n",
        "      ff7.append(f7)\n",
        "      ff8.append(f8)\n",
        "      ff9.append(f9)\n",
        "      ff10.append(f10)\n",
        "\n",
        "      ff11.append(f11)\n",
        "      ff12.append(f12)\n",
        "      ff13.append(f13)\n",
        "      ff14.append(f14)\n",
        "      ff15.append(f15)\n",
        "      ff16.append(f16)\n",
        "      ff17.append(f17)\n",
        "      ff18.append(f18)\n",
        "      ff19.append(f19)\n",
        "      ff20.append(f20)\n",
        "\n",
        "      ff21.append(f21)\n",
        "      ff22.append(f22)\n",
        "      ff23.append(f23)\n",
        "      ff24.append(f24)\n",
        "      ff25.append(f25)\n",
        "      ff26.append(f26)\n",
        "      ff27.append(f27)\n",
        "      ff28.append(f28)\n",
        "      ff29.append(f29)\n",
        "      ff30.append(f30)\n",
        "\n",
        "      ff31.append(f31)\n",
        "      ff32.append(f32)\n",
        "      ff33.append(f33)\n",
        "      ff34.append(f34)\n",
        "      ff35.append(f35)\n",
        "      ff36.append(f36)\n",
        "      ff37.append(f37)\n",
        "      ff38.append(f38)\n",
        "      ff39.append(f39)\n",
        "      ff40.append(f40)\n",
        "\n",
        "      ff41.append(f41)\n",
        "      ff42.append(f42)\n",
        "      ff43.append(f43)\n",
        "      ff44.append(f44)\n",
        "      ff45.append(f45)\n",
        "      ff46.append(f46)\n",
        "      ff47.append(f47)\n",
        "      ff48.append(f48)\n",
        "      ff49.append(f49)\n",
        "      ff50.append(f50)\n",
        "\n",
        "\n",
        "      ff51.append(f51)\n",
        "      ff52.append(f52)\n",
        "      ff53.append(f53)\n",
        "      ff54.append(f54)\n",
        "      ff55.append(f55)\n",
        "      ff56.append(f56)\n",
        "      ff57.append(f57)\n",
        "      ff58.append(f58)\n",
        "      ff59.append(f59)\n",
        "      ff60.append(f60)\n",
        "\n",
        "\n",
        "      ff61.append(f61)\n",
        "      ff62.append(f62)\n",
        "      ff63.append(f63)\n",
        "      ff64.append(f64)\n",
        "      ff65.append(f65)\n",
        "      ff66.append(f66)\n",
        "      ff67.append(f67)\n",
        "      ff68.append(f68)\n",
        "      ff69.append(f69)\n",
        "      ff70.append(f70)\n",
        "\n",
        "      ff71.append(f71)\n",
        "      ff72.append(f72)\n",
        "      ff73.append(f73)\n",
        "      ff74.append(f74)\n",
        "      ff75.append(f75)\n",
        "      ff76.append(f76)\n",
        "      ff77.append(f77)\n",
        "      ff78.append(f78)\n",
        "      ff79.append(f79)\n",
        "      ff80.append(f80)\n",
        "\n",
        "      ff81.append(f81)\n",
        "      ff82.append(f82)\n",
        "      ff83.append(f83)\n",
        "      ff84.append(f84)\n",
        "      ff85.append(f85)\n",
        "      ff86.append(f86)\n",
        "      ff87.append(f87)\n",
        "      ff88.append(f88)\n",
        "      ff89.append(f89)\n",
        "      ff90.append(f90)\n",
        "\n",
        "      ff91.append(f91)\n",
        "      ff92.append(f92)\n",
        "      ff93.append(f93)\n",
        "      ff94.append(f94)\n",
        "      ff95.append(f95)\n",
        "      ff96.append(f96)\n",
        "      ff97.append(f97)\n",
        "\n",
        "data2save = pd.DataFrame({'ff1':ff1,'ff2':ff2,'ff3':ff3,'ff4':ff4,'ff5':ff5,'ff6':ff6,'ff7':ff7,'ff8':ff8,'ff9':ff9,'ff10':ff10,\n",
        "'ff11':ff11,'ff12':ff12,'ff13':ff13,'ff14':ff14,'ff15':ff15,'ff16':ff16,'ff17':ff17,'ff18':ff18,'ff19':ff19,'ff20':ff20,\n",
        "'ff21':ff21,'ff22':ff22,'ff23':ff23,'ff24':ff24,'ff25':ff25,'ff26':ff26,'ff27':ff27,'ff28':ff28,'ff29':ff29,'ff30':ff30,\n",
        "'ff31':ff31,'ff32':ff32,'ff33':ff33,'ff34':ff34,'ff35':ff35,'ff36':ff36,'ff37':ff37,'ff38':ff38,'ff39':ff39,'ff40':ff40,\n",
        "'ff41':ff41,'ff42':ff42,'ff43':ff43,'ff44':ff44,'ff45':ff45,'ff46':ff46,'ff47':ff47,'ff48':ff48,'ff49':ff49,'ff50':ff50,\n",
        "'ff51':ff51,'ff52':ff52,'ff53':ff53,'ff54':ff54,'ff55':ff55,'ff56':ff56,'ff57':ff57,'ff58':ff58,'ff59':ff59,'ff60':ff60,\n",
        "'ff61':ff61,'ff62':ff62,'ff63':ff63,'ff64':ff64,'ff65':ff65,'ff66':ff66,'ff67':ff67,'ff68':ff68,'ff69':ff69,'ff70':ff70,\n",
        "'ff71':ff71,'ff72':ff72,'ff73':ff73,'ff74':ff74,'ff75':ff75,'ff76':ff76,'ff77':ff77,'ff78':ff78,'ff79':ff79,'ff80':ff80,\n",
        "'ff81':ff81,'ff82':ff82,'ff83':ff83,'ff84':ff84,'ff85':ff85,'ff86':ff86,'ff87':ff87,'ff88':ff88,'ff89':ff89,'ff90':ff90,\n",
        "'ff91':ff91,'ff92':ff92,'ff93':ff93,'ff94':ff94,'ff95':ff95,'ff96':ff96,'ff97':ff97})\n",
        "\n",
        "\n",
        "data2save.to_csv(\"Input_Dataset.csv\",index=False,header=True)\n",
        "\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data2ML = pd.read_csv(\"Input_Dataset.csv\")\n",
        "\n",
        "X = data2ML.drop('ff97', axis=1)\n",
        "y = data2ML['ff97']\n",
        "\n",
        "\n",
        "# y_t = np.array(data2ML['ff97'])\n",
        "# X_t = data2ML\n",
        "# X_t = data2ML.drop(['ff97'],axis=1)\n",
        "# X_t = np.array(X_t)\n",
        "\n",
        "# print(\"shape of Y :\"+str(y_t.shape))\n",
        "# print(\"shape of X :\"+str(X_t.shape))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dqrn29CAlYV8"
      },
      "outputs": [],
      "source": [
        "# AdaBoosting Testing\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "AdaBoosting = AdaBoostClassifier().fit(X, y)\n",
        "y_predicted = AdaBoosting.predict(X)\n",
        "\n",
        "confusion_matrix = metrics.confusion_matrix(y, y_predicted)\n",
        "\n",
        "Accuracy = metrics.accuracy_score(y, y_predicted)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion = confusion_matrix(y, y_predicted)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y, y_predicted)*100))\n",
        "\n",
        "print('Precision: {:.2f}'.format(precision_score(y, y_predicted, average='weighted')*100))\n",
        "print('Recall: {:.2f}'.format(recall_score(y, y_predicted, average='weighted')*100))\n",
        "print('F1-score: {:.2f}'.format(f1_score(y, y_predicted, average='weighted')*100))\n",
        "\n",
        "TP_class1 = confusion[2][2]\n",
        "TN_class1 = confusion[0][2]+confusion[1][2]\n",
        "print()\n",
        "precision_class1 = round(TP_class1/(TP_class1+TN_class1)*100,2)\n",
        "print('Percentage of positive predictions (class1) : ',precision_class1)\n",
        "\n",
        "print()\n",
        "print()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}